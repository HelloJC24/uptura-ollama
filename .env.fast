# Optimized Flask RAG API Configuration for Fast Streaming
# Copy this to .env for better performance

# Flask Settings
FLASK_HOST=0.0.0.0
FLASK_PORT=5000
FLASK_DEBUG=False

# Logging Settings (reduce logging for better performance)
LOG_LEVEL=WARNING

# Ollama Settings (optimized for speed)
OLLAMA_HOST=http://72.60.43.106:11434
OLLAMA_MODEL=phi3:mini
OLLAMA_TIMEOUT=10

# Redis Settings
REDIS_HOST=bngcpython-aiknow-myaa28
REDIS_PORT=6379
REDIS_PASSWORD=987654321
REDIS_DB=0
REDIS_TIMEOUT=2

# Embedding Model Settings (faster model)
EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG Settings (optimized for speed)
CHUNK_SIZE=150
CHUNK_OVERLAP=25
TOP_K=2
MIN_SIMILARITY=0.3
MAX_QUERY_LENGTH=500

# Response Settings (optimized for streaming)
ENABLE_STREAMING=True
CACHE_TTL=1800
MAX_CACHE_SIZE=500

# Conversation Settings (optimized)
ENABLE_CONVERSATIONS=True
MAX_CONVERSATION_HISTORY=8
CONVERSATION_TTL=43200
REQUIRE_USER_ID=False

# Optimized System Prompt (shorter for faster processing)
SYSTEM_PROMPT="You are a helpful assistant. Answer based on the provided context. If information is not available, say so briefly."